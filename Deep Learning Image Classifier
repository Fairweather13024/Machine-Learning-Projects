#GENERAL INTUITION. SEE PROJECT CODE BELOW DOC STRINGS#

#It is an object oriented implementation intuition
'''#Batches are useful implementations because
#1. The batches allow us to run multiple inputs at the same time, hence the algorithm learns more generally from all inputs
#2. It prevents from overfitting from the data
#3. The larger the batch, the more simple calculations that a GPU can do among its thousands of cores

import numpy as np

X=[[1,2,3,2.5],[2,5,1,2],[-1.5,2.7,3.3,-0.8]]

weights=[]
#bias=
output=[]
# output=np.dot(weights.T, inputs)+ bias
np.random.seed(0)

#We are creating an object to create weights between -1 and 1, the biases as an array and the output for each layer we pass forward
#We also create a biases 1 dimensional array which is as long as the no of outputs
class Layer_Dense:
    #This initializes the weights and the biases
    def __init__(self, n_inputs, n_neurons):
        # Number of neurons you want
        self.weights=0.1*(np.random.randn(n_inputs, n_neurons))#The switched order of n_inputs and n_neurons eliminates the need to transpose

        self.biases=np.zeros((1,n_neurons)) #Tuple
    #This initializes the output which returns an array
    def forward(self):
        self.output = np.dot(X, self.weights) + self.biases
print(np.random.randn(4,3))

#We will use the rectified linear activation function to model the relationships in the inputs
#This ReLU function is only activated when the input is greater than 0 max(0,output 'z')

class Activation_function:
    def forward (self, X):
        self.output=for i in X:
            if i > 0:
                output.append(i)
            elif i >= 0:
                output.append(0)
        return self.output

layer1=Layer_Dense(3,4)#Takes data from 3 input nodes and relays it to 4 neurons
layer2=Layer_Dense(4,5)#Takes from the previous 4 nodes, and outputs to 5 neurons

activation_layer1=Activation_function()
layer1.forward(X)#Running the inputs through the first forward propagation
activation_layer1.forward(layer1.output)#This takes out all of the negative values from the output by making them 0
print(activation_layer1.output)


#Back Propagation
#This is the algorithm that determines how much a single training example would like to alter the weights and biases for accurate output
#In essence we want to determine the alterations that would lead to the most rapid gradient descent
#Not that the implementation of this optimization may be hindered by local minima
#Now we can implement an appropriate loss function depending on our data (outliers), time efficiency of gradient descent and the confidence of predictions

#Log Loss function/cross entropy

def log_loss(X,y): # y is the labels that the data will be fitted to
    m=y.shape[0]
    p=softmax(X)
    log_likelihood = -np.long(p[range(m),y])
    loss=np.sum(log_likelihood)/m
    return loss

#Remember that we will update our weights and biases based on the negative gradient of
#the cost function in order to minimize the cost which is an array of each change that should be made for each outut
#in order to have a reliable classification
#We can quicken this up with an optimization technique.
#We can shuffle the data and split it into tiny batches to calculate the costs of each batch
#Therefore, each negative gradient will be a step in the right direction
#This would be an implementation of a greedy algorithm, hence you may jeopardise your accuracy, hence warranting many random shuffles
#You may also be interested in a simulated annealing implementation
'''


import numpy as np
import h5py #If data is stored on a H5 file (image processing)
import matplotlib.pyplot as plt
from testCases_v4a import *
from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward

%matplotlib inline
plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'

#Jupyter things
%load_ext autoreload
%autoreload 2

np.random.seed(1) #Ensures that the random values that are generated are replicable

#THIS WILL INITIALIZE PARAMS FOR ONLY A 2 LAYER MODEL, FOR 'N' LAYERS, SEE NEXT FUNCTION
def initialize_parameters(n_x, n_h, n_y):
    """
    Arguments:
    n_x -- size of the input layer
    n_h -- size of the hidden layer
    n_y -- size of the output layer

    Returns:
    parameters -- python dictionary containing your parameters:
                    W1 -- weight matrix of shape (n_h, n_x)
                    b1 -- bias vector of shape (n_h, 1)
                    W2 -- weight matrix of shape (n_y, n_h)
                    b2 -- bias vector of shape (n_y, 1)
    """

    np.random.seed(1)


    W1 = np.random.randn(n_h, n_x) * 0.01
    b1 = np.zeros((n_h, 1))
    W2 = np.random.randn(n_y, n_h) * 0.01
    b2 = np.zeros((n_y, 1))

#Assertions to ensure that we do not generate shape errors
#They will throw an error if the condition is not met
    assert (W1.shape == (n_h, n_x))
    assert (b1.shape == (n_h, 1))
    assert (W2.shape == (n_y, n_h))
    assert (b2.shape == (n_y, 1))

    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}

    return parameters

#iNITIALIZES PARAMS FOR 'N' LAYERS (MORE ROBUST AND SCALABLE)
def initialize_parameters_deep(layer_dims):
    """
    Arguments:
    layer_dims -- Array containing the dimensions of each layer in our network

    Returns:
    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])
                    bl -- bias vector of shape (layer_dims[l], 1)
    """

    np.random.seed(3)
    parameters = {}
    L = len(layer_dims)  # number of layers in the network

    for l in range(1, L):

        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01
        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))


        assert (parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))
        assert (parameters['b' + str(l)].shape == (layer_dims[l], 1))

    return parameters


#Initializing forward propagation function
#Initializing a cache to store the values of the weights and biases for back propagation
#The linear function vectorized over the example inputs implements:
#  Z[l]=W[l]A[l-1]+b[l]
#A in this case is the input (given by the activation of the hidden layers

def linear_forward(A, W, b):
    """
    Implement the linear part of a layer's forward propagation.

    Arguments:
    A -- activations from previous layer (or input data): (size of previous layer, number of examples)
    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)
    b -- bias vector, numpy array of shape (size of the current layer, 1)

    Returns:
    Z -- the input of the activation function, also called pre-activation parameter
    cache -- a python tuple containing "A", "W" and "b" ; stored for computing the backward pass efficiently
    """

    Z = np.dot(W, A) + b


    assert (Z.shape == (W.shape[0], A.shape[1]))
    cache = (A, W, b) #creates a tuple of the inputs, weights and biases for back propagatoin

    return Z, cache

#Implements A[l]=g(Z[l])=g(W[l]A[l-1]+b[l])
#g is the activation, which represents either a ReLu for the hidden layers, or a sigmoid activaton for the output layer
def linear_activation_forward(A_prev, W, b, activation):
    """
    Implement the forward propagation for the LINEAR->ACTIVATION layer

    Arguments:
    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)
    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)
    b -- bias vector, numpy array of shape (size of the current layer, 1)
    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"

    Returns:
    A -- the output of the activation function, also called the post-activation value
    cache -- a python tuple containing "linear_cache" and "activation_cache";
             stored for computing the backward pass efficiently
    """

    if activation == "sigmoid":
        # Inputs: "A_prev, W, b". Outputs: "A, activation_cache".

        Z, linear_cache = linear_forward(A_prev, W, b)
        A, activation_cache = sigmoid(Z)


    elif activation == "relu":
        # Inputs: "A_prev, W, b". Outputs: "A, activation_cache".

        Z, linear_cache = linear_forward(A_prev, W, b)
        A, activation_cache = relu(Z)


    assert (A.shape == (W.shape[0], A_prev.shape[1]))
    cache = (linear_cache, activation_cache)

    return A, cache

#Here we loop over the layers (l-1) to run the forward propagation step for l layers
#We also store the cache data
def L_model_forward(X, parameters):
    """
    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation

    Arguments:
    X -- data, numpy array of shape (input size, number of examples)
    parameters -- output of initialize_parameters_deep()

    Returns:
    AL -- last post-activation value
    caches -- list of caches containing:
                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)
    """

    caches = []
    A = X
    L = len(parameters) // 2  # number of layers in the neural network

    # Implement [LINEAR -> RELU]*(L-1). Add "cache" to the "caches" list.
    for l in range(1, L):
        A_prev = A

        A, cache = linear_activation_forward(A_prev, parameters["W" + str(l)], parameters["b" + str(l)],
                                             activation='relu')
        caches.append(cache)


    # Implement LINEAR -> SIGMOID. Add "cache" to the "caches" list.

    AL, cache = linear_activation_forward(A, parameters["W" + str(L)], parameters["b" + str(L)], activation='sigmoid')
    caches.append(cache)


    assert (AL.shape == (1, X.shape[1]))

    return AL, caches

#Cost function to inform the adjustment of our weights and biases
#This is the log loss cost function

def compute_cost(AL, Y):
    """
    Implement the cost function defined by equation (7).

    Arguments:
    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)
    Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)

    Returns:
    cost -- cross-entropy cost
    """

    m = Y.shape[1]

    # Compute loss from aL and y.

    cost = (-1. / m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply((1 - Y), np.log(1 - AL)))


    cost = np.squeeze(cost)  # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).
    assert (cost.shape == ())

    return cost

#BACK PROPAGATION
#Here we are calculating the gradient of the loss function with respoect to the parameters
#We will then be implementing gradient descent
#The equations are:
#dW[l]=(1/m)dZ[l]A[l-1]T
#db[l]=(1-m)*(sum from 1 to m)dZ[l]i
#dA[l-1]=W[l]TdZ[l]

def linear_backward(dZ, cache):
    """
    Implement the linear portion of backward propagation for a single layer (layer l)

    Arguments:
    dZ -- Gradient of the cost with respect to the linear output (of current layer l)
    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer

    Returns:
    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
    dW -- Gradient of the cost with respect to W (current layer l), same shape as W
    db -- Gradient of the cost with respect to b (current layer l), same shape as b
    """
    A_prev, W, b = cache
    m = A_prev.shape[1]


    dW = (1. / m) * np.dot(dZ, cache[0].T)
    db = (1. / m) * np.sum(dZ, axis=1, keepdims=True)
    dA_prev = np.dot(cache[1].T, dZ)


    assert (dA_prev.shape == A_prev.shape)
    assert (dW.shape == W.shape)
    assert (db.shape == b.shape)

    return dA_prev, dW, db

#This function merges the functionality of the previous 2 functions
#With g() as the umbrella term for activation functions dZ[l]=dA[l] * g'(Z[l])

def linear_activation_backward(dA, cache, activation):
    """
    Implement the backward propagation for the LINEAR->ACTIVATION layer.

    Arguments:
    dA -- post-activation gradient for current layer l
    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently
    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"

    Returns:
    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
    dW -- Gradient of the cost with respect to W (current layer l), same shape as W
    db -- Gradient of the cost with respect to b (current layer l), same shape as b
    """
    linear_cache, activation_cache = cache

    if activation == "relu":

        dZ = relu_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)


    elif activation == "sigmoid":

        dZ = sigmoid_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)


    return dA_prev, dW, db


#*****This here function has a problem with the shape of dZ *****
#dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL
#We are finding the gradients at post-activation (dAL), looping over all layers and storing the derivatives as dA, DW and db
#grads['dW' +str(l)]=dW[l] which would store layer to in grads as ['dW2

def L_model_backward(AL, Y, caches):
    """
    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group

    Arguments:
    AL -- probability vector, output of the forward propagation (L_model_forward())
    Y -- true "label" vector (containing 0 if non-cat, 1 if cat)
    caches -- list of caches containing:
                every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)
                the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1])

    Returns:
    grads -- A dictionary with the gradients
             grads["dA" + str(l)] = ...
             grads["dW" + str(l)] = ...
             grads["db" + str(l)] = ...
    """
    grads = {}
    L = len(caches)  # the number of layers
    m = AL.shape[1]
    Y = Y.reshape(AL.shape)  # after this line, Y is the same shape as AL

    # Initializing the backpropagation
    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))

    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: "dAL, current_cache". Outputs: "grads["dAL-1"], grads["dWL"], grads["dbL"]

    current_cache = caches[-1]
    grads["dA" + str(L)], grads["dW" + str(L)], grads["db" + str(L)] = linear_activation_backward(dAL, current_cache,
                                                                                                  activation="sigmoid")


    # Loop from l=L-2 to l=0
    for l in reversed(range(L - 1)):
        # lth layer: (RELU -> LINEAR) gradients.
        # Inputs: "grads["dA" + str(l + 1)], current_cache". Outputs: "grads["dA" + str(l)] , grads["dW" + str(l + 1)] , grads["db" + str(l + 1)]

        current_cache = caches[1]
        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads["dA" + str(l + 2)], current_cache,
                                                                    activation="relu")
        grads["dA" + str(l + 1)] = dA_prev_temp
        grads["dW" + str(l + 1)] = dW_temp
        grads["db" + str(l + 1)] = db_temp

    return grads

#WIth the gradients computed, we can now update W and b using gradient descent
#W[l]=W[l]=adW[l]
#b[l]=b[l] - adb[l]
#a is the learning rate which should be small to find optimum
def update_parameters(parameters, grads, learning_rate):
    """
    Update parameters using gradient descent

    Arguments:
    parameters -- python dictionary containing your parameters
    grads -- python dictionary containing your gradients, output of L_model_backward

    Returns:
    parameters -- python dictionary containing your updated parameters
                  parameters["W" + str(l)] = ...
                  parameters["b" + str(l)] = ...
    """

    L = len(parameters) // 2  # number of layers in the neural network

    # Update rule for each parameter. Use a for loop.

    for l in range(L):
        parameters["W" + str(l + 1)] = parameters["W" + str(l + 1)] - learning_rate * grads["dW" + str(l + 1)]
        parameters["b" + str(l + 1)] = parameters["b" + str(l + 1)] - learning_rate * grads["db" + str(l + 1)]

    return parameters

'''Now we will explore the data we have (cat images) and we will run the above helper functions accordingly'''
train_x_orig, train_y, test_x_orig, test_y, classes = load_data()
#Gives an example of the data
index = 10
plt.imshow(train_x_orig[index])
print ("y = " + str(train_y[0,index]) + ". It's a " + classes[train_y[0,index]].decode("utf-8") +  " picture.")
m_train = train_x_orig.shape[0]
num_px = train_x_orig.shape[1]
m_test = test_x_orig.shape[0]

print ("Number of training examples: " + str(m_train))
print ("Number of testing examples: " + str(m_test))
print ("Each image is of size: (" + str(num_px) + ", " + str(num_px) + ", 3)")
print ("train_x_orig shape: " + str(train_x_orig.shape))
print ("train_y shape: " + str(train_y.shape))
print ("test_x_orig shape: " + str(test_x_orig.shape))
print ("test_y shape: " + str(test_y.shape))

#Vectorizing the images
#All of the images come in three layers, RGB, each of which has a numeric representation
#We vectorize so that we can run the code to find trends without being taxing on computation
# Reshape the training and test examples
train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The "-1" makes reshape flatten the remaining dimensions
test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T

# Standardize data to have feature values between 0 and 1.
train_x = train_x_flatten/255.
test_x = test_x_flatten/255.

print ("train_x's shape: " + str(train_x.shape))
print ("test_x's shape: " + str(test_x.shape))


### CONSTANTS ###
layers_dims = [12288, 20, 7, 5, 1] #  4-layer model for computing speed


def L_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):  # lr was 0.009
    """
    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.

    Arguments:
    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)
    Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)
    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).
    learning_rate -- learning rate of the gradient descent update rule
    num_iterations -- number of iterations of the optimization loop
    print_cost -- if True, it prints the cost every 100 steps

    Returns:
    parameters -- parameters learnt by the model. They can then be used to predict.
    """

    np.random.seed(1)
    costs = []  # keep track of cost

    # Parameters initialization. (≈ 1 line of code)

    parameters = initialize_parameters_deep(layers_dims)


    # Loop (gradient descent)
    for i in range(0, num_iterations):

        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.

        AL, caches = L_model_forward(X, parameters)


        # Compute cost.
        cost = compute_cost(AL, Y)

        # Backward propagation.
        grads = L_model_backward(AL, Y, caches)

        # Update parameters W and b
        parameters = update_parameters(parameters, grads, learning_rate)


        # Print the cost every 100 training example
        if print_cost and i % 100 == 0:
            print("Cost after iteration %i: %f" % (i, cost))
        if print_cost and i % 100 == 0:
            costs.append(cost)

    # plot the cost
    plt.plot(np.squeeze(costs))
    plt.ylabel('cost')
    plt.xlabel('iterations (per hundreds)')
    plt.title("Learning rate =" + str(learning_rate))
    plt.show()

    return parameters

parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)
pred_train = predict(train_x, train_y, parameters)
pred_test = predict(test_x, test_y, parameters)

#Now we will analyze the mislabeled images
print_mislabeled_images(classes, test_x, test_y, pred_test)

#It is noted that the algorithm performs poorly with cat pictures that:
#Have the cat in an unusual position
#The cat is backdropped against a background of similar color
#Unusual cat color
#Unusual camera angle
#The image brightness is very peculiar
#The cat is very latge or small in th eimage
